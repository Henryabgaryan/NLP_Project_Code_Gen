{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzaLVqwwrn0P"
      },
      "source": [
        "# IASD NLP Project \n",
        "\n",
        "#Code generation with language models\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Authors: \n",
        "\n",
        "> Madina Babazhanova\n",
        "\n",
        "> Jiheng \n",
        "\n",
        "> Henrik Abgaryan\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8v3ye-CrDAN",
        "outputId": "c812529e-944c-4995-fa15-90bb5904d360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.29.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Collecting responses<0.19 (from datasets)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "34bb4efaa82549239e54cf76e521c966",
            "5f94b4d7e7cf4c4ab2835ca70c9616bf"
          ]
        },
        "id": "Ukeb0plNg9UF",
        "outputId": "4f26feb6-9f35-40b3-e9af-e12826c33f5e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34bb4efaa82549239e54cf76e521c966",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.23k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f94b4d7e7cf4c4ab2835ca70c9616bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/7.54k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:datasets.builder:No config specified, defaulting to: github-code/all-all\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'code': '\\'use strict\\';\\n\\nvar clear          = require(\\'es5-ext/array/#/clear\\')\\n  , eIndexOf       = require(\\'es5-ext/array/#/e-index-of\\')\\n  , setPrototypeOf = require(\\'es5-ext/object/set-prototype-of\\')\\n  , callable       = require(\\'es5-ext/object/valid-callable\\')\\n  , d              = require(\\'d\\')\\n  , ee             = require(\\'event-emitter\\')\\n  , Symbol         = require(\\'es6-symbol\\')\\n  , iterator       = require(\\'es6-iterator/valid-iterable\\')\\n  , forOf          = require(\\'es6-iterator/for-of\\')\\n  , Iterator       = require(\\'./lib/iterator\\')\\n  , isNative       = require(\\'./is-native-implemented\\')\\n\\n  , call = Function.prototype.call, defineProperty = Object.defineProperty\\n  , SetPoly, getValues;\\n\\nmodule.exports = SetPoly = function (/*iterable*/) {\\n\\tvar iterable = arguments[0];\\n\\tif (!(this instanceof SetPoly)) return new SetPoly(iterable);\\n\\tif (this.__setData__ !== undefined) {\\n\\t\\tthrow new TypeError(this + \" cannot be reinitialized\");\\n\\t}\\n\\tif (iterable != null) iterator(iterable);\\n\\tdefineProperty(this, \\'__setData__\\', d(\\'c\\', []));\\n\\tif (!iterable) return;\\n\\tforOf(iterable, function (value) {\\n\\t\\tif (eIndexOf.call(this, value) !== -1) return;\\n\\t\\tthis.push(value);\\n\\t}, this.__setData__);\\n};\\n\\nif (isNative) {\\n\\tif (setPrototypeOf) setPrototypeOf(SetPoly, Set);\\n\\tSetPoly.prototype = Object.create(Set.prototype, {\\n\\t\\tconstructor: d(SetPoly)\\n\\t});\\n}\\n\\nee(Object.defineProperties(SetPoly.prototype, {\\n\\tadd: d(function (value) {\\n\\t\\tif (this.has(value)) return this;\\n\\t\\tthis.emit(\\'_add\\', this.__setData__.push(value) - 1, value);\\n\\t\\treturn this;\\n\\t}),\\n\\tclear: d(function () {\\n\\t\\tif (!this.__setData__.length) return;\\n\\t\\tclear.call(this.__setData__);\\n\\t\\tthis.emit(\\'_clear\\');\\n\\t}),\\n\\tdelete: d(function (value) {\\n\\t\\tvar index = eIndexOf.call(this.__setData__, value);\\n\\t\\tif (index === -1) return false;\\n\\t\\tthis.__setData__.splice(index, 1);\\n\\t\\tthis.emit(\\'_delete\\', index, value);\\n\\t\\treturn true;\\n\\t}),\\n\\tentries: d(function () { return new Iterator(this, \\'key+value\\'); }),\\n\\tforEach: d(function (cb/*, thisArg*/) {\\n\\t\\tvar thisArg = arguments[1], iterator, result, value;\\n\\t\\tcallable(cb);\\n\\t\\titerator = this.values();\\n\\t\\tresult = iterator._next();\\n\\t\\twhile (result !== undefined) {\\n\\t\\t\\tvalue = iterator._resolve(result);\\n\\t\\t\\tcall.call(cb, thisArg, value, value, this);\\n\\t\\t\\tresult = iterator._next();\\n\\t\\t}\\n\\t}),\\n\\thas: d(function (value) {\\n\\t\\treturn (eIndexOf.call(this.__setData__, value) !== -1);\\n\\t}),\\n\\tkeys: d(getValues = function () { return this.values(); }),\\n\\tsize: d.gs(function () { return this.__setData__.length; }),\\n\\tvalues: d(function () { return new Iterator(this); }),\\n\\ttoString: d(function () { return \\'[object Set]\\'; })\\n}));\\ndefineProperty(SetPoly.prototype, Symbol.iterator, d(getValues));\\ndefineProperty(SetPoly.prototype, Symbol.toStringTag, d(\\'c\\', \\'Set\\'));\\n', 'repo_name': 'Socratacom/socrata-europe', 'path': 'wp-content/themes/sage/node_modules/asset-builder/node_modules/main-bower-files/node_modules/vinyl-fs/node_modules/glob-stream/node_modules/unique-stream/node_modules/es6-set/polyfill.js', 'language': 'JavaScript', 'license': 'gpl-2.0', 'size': 2730}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'code': \"import mod189 from './mod189';\\nvar value=mod189+1;\\nexport default value;\\n\",\n",
              " 'repo_name': 'MirekSz/webpack-es6-ts',\n",
              " 'path': 'app/mods/mod190.js',\n",
              " 'language': 'JavaScript',\n",
              " 'license': 'isc',\n",
              " 'size': 73}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\")\n",
        "print(next(iter(ds)))\n",
        "\n",
        "#OUTPUT:\n",
        "{\n",
        " 'code': \"import mod189 from './mod189';\\nvar value=mod189+1;\\nexport default value;\\n\",\n",
        " 'repo_name': 'MirekSz/webpack-es6-ts',\n",
        " 'path': 'app/mods/mod190.js',\n",
        " 'language': 'JavaScript',\n",
        " 'license': 'isc',\n",
        " 'size': 73\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X1iZI39q7kg"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"codeparrot/codeparrot\")\n",
        "\n",
        "inputs = tokenizer(\"def hello_world():\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHt-3QrrSeI"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=\"codeparrot/codeparrot\")\n",
        "outputs = pipe(\"def hello_world():\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc_yqIMIrVgz"
      },
      "source": [
        "#3 - Code model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIsoxPS3rJ-7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from evaluate import load\n",
        "\n",
        "human_eval = load_dataset(\"openai_humaneval\")\n",
        "code_eval_metric = load(\"code_eval\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyH0UPnlrdGq"
      },
      "outputs": [],
      "source": [
        "test_cases = [\"assert add(2,3)==5\"]\n",
        "candidates = [[\"def add(a,b): return a*b\", \"def add(a, b): return a+b\"]]\n",
        "pass_at_k, results = code_eval_metric.compute(references=test_cases, predictions=candidates, k=[1, 2])\n",
        "print(pass_at_k)\n",
        "{'pass@1': 0.5, 'pass@2': 1.0}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHEr7H4SrgUh"
      },
      "outputs": [],
      "source": [
        "def truncate_number(number: float) -> float:\n",
        "    \"\"\" Given a positive floating point number, it can be decomposed into\n",
        "    and integer part (largest integer smaller than given number) and decimals\n",
        "    (leftover part always smaller than 1).\n",
        "\n",
        "    Return the decimal part of the number.\n",
        "    >>> truncate_number(3.5)\n",
        "    0.5\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWVTZV3VbED9"
      },
      "source": [
        "#Full training pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbgmoK0ErgzX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "ds = load_dataset(\"codeparrot/github-code\", streaming=True, split=\"train\")\n",
        "\n",
        "# Load the GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Define the training loop\n",
        "def train_gpt(model, tokenizer, data_loader, num_epochs=10, learning_rate=0.0001):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for batch in data_loader:\n",
        "            # Tokenize the batch of code snippets\n",
        "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            labels = inputs[\"input_ids\"].clone()\n",
        "\n",
        "            # Shift the labels to the right by one token\n",
        "            labels[:, :-1] = inputs[\"input_ids\"][:, 1:]\n",
        "            labels[:, -1] = tokenizer.eos_token_id\n",
        "\n",
        "            # Move the inputs and labels to the GPU\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Generate code snippets with the model\n",
        "            outputs = model(**inputs, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # Backpropagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "# Define the data loader\n",
        "class GithubCodeDataset(torch.utils.data.IterableDataset):\n",
        "    def __init__(self, ds):\n",
        "        self.ds = ds\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            for ex in self.ds:\n",
        "                yield ex[\"code\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZbOSEIWYkwJ"
      },
      "outputs": [],
      "source": [
        "dataset = GithubCodeDataset(ds)\n",
        "data_loader = DataLoader(dataset, batch_size=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyO09PLLcKJz"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HzxS4qOcB91"
      },
      "outputs": [],
      "source": [
        "train_gpt(model, tokenizer, data_loader, num_epochs=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}